Survey design check list

- [Survey interface【形式】](#survey-interface形式)
  - [It needs to look good](#it-needs-to-look-good)
  - [First page](#first-page)
  - [Thank you & contact information](#thank-you--contact-information)
  - [Interviewer instructions](#interviewer-instructions)
  - [Numbering questions](#numbering-questions)
  - [Fitting questions on pages/screen](#fitting-questions-on-pagesscreen)
  - [Ordering questions](#ordering-questions)
  - [Types of questions](#types-of-questions)
- [Questions and response options【内容】](#questions-and-response-options内容)
  - [Ensure consistent response](#ensure-consistent-response)
  - [4 reasons why respondents fail to answer factual questions and how to do](#4-reasons-why-respondents-fail-to-answer-factual-questions-and-how-to-do)
  - [Stylistic responding and how to do](#stylistic-responding-and-how-to-do)
  - [Subjective questions (increase validity)](#subjective-questions-increase-validity)
  - [Pitfalls/check list](#pitfallscheck-list)
  - [Redflag words in question wording](#redflag-words-in-question-wording)
- [Web survey design settings 【设置】](#web-survey-design-settings-设置)
  - [Avoid breakoffs](#avoid-breakoffs)
  - [Distribute web survey](#distribute-web-survey)
  - [Basic design](#basic-design)
  - [Question setting](#question-setting)
  - [Response format & scales](#response-format--scales)
  - [Response heuristics](#response-heuristics)



# Survey interface【形式】

## It needs to look good

- should be well-organized and professional
- should convey seriousness

## First page

- Put your university at the beginning
- should contain: name, where are you from, the purpose of the study, conditions (voluntary, anonymous, etc.)
>  *To begin, just scroll down and click the "next" button.*

## Thank you & contact information

- put at the beginning and the end of the survey
- make them serious
- you don't want the respondents to go back
- all contact information should look badass; don't look stupid and silly

 ## Interviewer instructions

- in different forms (so that instructions can be easily distinguished from other parts of the survey)
- put them to where it's going to be used
  - instructions about how to read the question should be put before the question
  - instructions about how much responses should be put before the question
  - Instructions about how to code should be put after the question

## Numbering questions

- always number questions
- number subparts with subscripts

> Reasons:
>
> - ability to get complaints/concerns about questions
> - satisficing: the respondents would feel like they are making progress

## Fitting questions on pages/screen

- don't break questions outside a page
- don't put a small question after a long question (otherwise the respondents will skip it by accident)
- unclusterred (otherwise it will look intimidating)

## Ordering questions

> Goals:
>
> - motivate the respondents to keep going (you don't want them to stop)
> - minimize question-order effects (the effect of one question on another one)

- the first question **should not be demographic questions**
  - the first question should be interesting
  - should concern you and should be related to the research
  - should not be income questions (most sensitive, respondents will quit or skip it)

> Reasons:
>
> - demographic questions have two purposes: make sure the eligibility (screening/attrition), and seeing what the respondents are like
> - if they feel like they are being screened, they lie
> - reduce anger
> - demographic information form

- leave some question before demographic questions
  - these questions should interest them
  - should be questions that everybody can answer
  - should be done in a non-threatenning way
  - should not be income questions (most sensitive, respondents will quit or skip it)

## Types of questions 

 [Dillman, Smyth, and Christian, 2014, p. 110](https://www.wiley.com/en-us/Internet%2C+Phone%2C+Mail%2C+and+Mixed+Mode+Surveys%3A+The+Tailored+Design+Method%2C+4th+Edition-p-9781118456149)

- Open-ended questions

> - advantages: provide the richest detail; closely proximate what people actually think; great for pretest (if you don't know what the relevant response options will be, do a pretest using open-ended questions and figure them out); sometimes answers theoretically too long to provide (there may be hundreds of possible reponses); allow for unanticipated answers. 
>
> - disadvantages: put higher burden on respondents (tend to skip them); time consuming; contain irrelevant words; hard to answer on smart phones or on telephone; contain errors; difficult to code and interpret.

- close-ended questions (99% of the cases they are the best options)

> - advantages: more practical; alreadly coded; more reliable for respondents and researchers (they don't love to interpret it)
> - disadvantages: accuracy concerns; may leave out important categories; lose detail

- close-ended question with an open-ended stem (other option)

> - question stem and response options should match: repondents should be able to guess responses types. See [19 in Pitfalls/check list](#pitfallscheck-list).
> - Format affects responses (See split ballot experiment from [Pew Research, 2008](https://www.questionpro.com/blog/what-are-open-ended-questions/))
>   - People who got the close-ended question with an other option were always more likely to select from the list, rather than volunteer to write anything else 
>   - almost nobody (8%) who got the close-ended question with other category used the "other" category (vs 43% of those who got open-ended questions)
> - Open-ended question generated more missing data ([Reja et al., 2003](https://www.researchgate.net/publication/242672718_Open-ended_vs_Close-ended_Questions_in_Web_Questionnaires))
>   - 12% uninterpretable in open-ended questions
>   - 0.8%/0% missing in close-ended questions vs 41.3% missing (due to skipping or not interpretable) in open-ended questions

- matrix questions with Likert statements (a series of questions sharing the same response scale, like level of agreement/acceptability/support/frequencies)
  - first couple of items should be in opposite direction so that the degree of one differs with the degree of another (if in the first item higher scores indicates more, in the second item higher scores should indicate less)
  - shading to differentiate the rows
  - reserve matrix questions for scales (that are related to each other); if the questions are not related to each other, not recommmended (meta analyses showed mixed results, the effects are probably small, but not recommended to do so) because respondents get the mindset that the items are related to each other

> - advantages: use space more efficiently; easier for respondents to do; let respondents see the items being parallel (they are measuring the same thing); set the items being comparable
> - disadvantages: increase the risk that people just respond in the same way to all the items (that's way items in the same directions tend to fail); increase missingness (people may accidentally skip a row or answer in the wrong row; that's why we should shade the rows)

- contingency questions (questions asked to different subgroups based on anwers to some screening questions)

> - advantages: respondents won't answer irrelevant questions
> - disadvantages: hard to do (skipping causes huge errors) unless using computers; 30% of the people get it wrong ([Dillman, Smyth, and Christian, 2014, p. 106](https://www.wiley.com/en-us/Internet%2C+Phone%2C+Mail%2C+and+Mixed+Mode+Surveys%3A+The+Tailored+Design+Method%2C+4th+Edition-p-9781118456149)); the good news is computer does it automatically and most of the times you can frame the questions in a way that they don't have to be contingent

---



# Questions and response options【内容】

[Fowler, 2013, Chapter 6](https://uk.sagepub.com/sites/default/files/upm-binaries/23856_Chapter6.pdf)

[Bradburn, Sudman, and Wansink, 2004](https://www.amazon.com/Asking-Questions-Definitive-Questionnaire-Questionnaires/dp/0787970883/ref=sr_1_6?dchild=1&keywords=asking+questions&qid=1617482194&s=books&sr=1-6)

[Schuman and Presser, 1996](https://www.amazon.com/SCHUMAN-ATTITUDESURVEYS-EXPERIMENTS-Experiments-Quantitative-dp-0761903593/dp/0761903593/ref=mt_other?_encoding=UTF8&me=&qid=1617482376)



A bad example:

> [Harvard 2017 poll about how we should handle Iran Nuke Deal](https://theintercept.com/2017/10/28/renegotiate-iran-deal-harvard-harris-poll-mark-penn/):
>
> Some people say that the Iran nuclear deal is not perfect and the Iranians are building up their nuclear capability secretly, but we should not rock the boat now and just let it all slide along. Others say if Iranians are not compliant we have to call them out on it and push to renegotiate the deal with real verification. What would be your preferred course of action?
>
> - clearly biased because it assumed Iran violated the deal
> - "Some...others" issue, see [17 in Pitfalls/check list](#pitfallscheck-list).

## Ensure consistent response

improve reliability

1. make sure each respondent is asked the same question
   
   - interviews should be entirely scripted (otherwise different things will be said to different people, and selected explanations & process will only be given to a certain people)
  
2. wording should be complete (otherwise different people will interpret the question differently)

3. wording should be simple, straightforward, and as short as possible (otherwise comprehension breaks down and people will be responding to different questions. The more complex the question, the more difficult the comprehension, the more you are going to have variation in ways people comprehend the question)

4. consistent meaning: words need to mean the same thing for everybody

   - avoid terms that are not universally understood/jagon/slang

     > eg. "status offense," "capitalism"

   - if terms are vague or ambiguous, people are going to define them differently

     > eg. "gun control" will be vague (handgun or other guns? how long the control will last?)

   - if terms are not vague but have inconsistent meaning for different people, avoid them

     > eg. "neighborhood" does not have a clear definition, and has different meanings for people living in different  (urban/rural) areas
     >
     > eg. "breakfast" has different meaning for different people, some people interpret it as eggs and bread, others may interpret it as smoothie 

5. for open-ended questions, clarify acceptable type of answers (words or numbers)

   > eg. Q: When was the first time you were arrested? should specify what types of answer is wanted, a specific year, a long time ago, or when I was a child? 
   >
   > eg. box size changes the way people respond; should use smaller box for MM, larger box for YYYY
   >
   > <textarea cols="2" rows="1">  </textarea>  MM <textarea cols="4" rows="1">  </textarea> YYYY 

6. avoid double-barrel questions (they reduce reliability, people will choose one aspect to answer, and different people answer different questions)
7. avoid biased questions

## 4 reasons why respondents fail to answer factual questions and how to do

1. respondents don't understand the question
   - make it easier
2. respondents don't know the answer
   - ask what people would know
3. respondents cannot recall
   - Make it salient; recent; add cues/hints
   - the reference period should be tailored to subjects: should be a likely to recall period of time; for things that frequently happen, use shorter reference period; for things that rarely happen but salient, you can use longer reference period
4. they don't want to tell you （social desirability bias)
   - use self-administered survey
   - tell the respondents the survey is anonymous or at least confidential



## Stylistic responding and how to do

respond in similar consistent way, regardless of the context

| Response style                                               | Bias                                                         |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| Acquiescence response style [XXXOOO] ([Pickett & Baker, 2014](https://onlinelibrary.wiley.com/doi/abs/10.1111/1745-9125.12035))<br />tendency to affirmatively choose (3 agree options), regardless of the context; 10% people use acquiescence response | positive bias<br /> brings positive error variance and waters down the origianal negative relationship, push it to 0 |
| Disacquiescence response style [OOOXXX]<br />disagree consistently | positive bias<br />introduce positive error variance         |
| midpoint response style [XXXOXXX]                            | positive error<br />introduce positive variance, increase correlations |
| extreme response style<br />go for the most extreme response | Depends                                                      |
| mild response style<br />always give middle response, never use extreme response | Depends                                                      |

- use balanced scale to push to the middle so that it doesn't have an effect (does not address midpoint, but it is not as common)

- representative corresponsive index/representative indicator corresponsive style index

  - construct groups of uncorrelated items, count the stylistic responses, make indicators, and residualize whatever scales you are working with (regress DV on indicators and save the residuals, remove the variance of the scale due to response styles; or use SEM to measure them)

    >  eg. *I like Apples more than oranges.* 



## Subjective questions (increase validity)

asking about attitudes/perceptions/believes

shoot for two types of validity

1. face validity: look reasonable on its face; look like it's measuring what you want to measure

2. content validity: cover full domain of a construct

   - ask more than 1 question, and combine them to create a scale (any given item has some randomness in it)

   - make sure all dimensions of a concept are covered and all subcategories are covered

     > eg. [GSS](https://gssdataexplorer.norc.org/trends/Civil%20Liberties?measure=fear) question for fear of crime (*Will you be afraid to walk alone at night?*) is highly criticized because it does not mention crime
     >
     > eg. fear of crime scale should consist of 4 -5 questions covering violent/nonviolent, severe/nonsevere crimes ([Ferraro, 1995](https://books.google.com/books/about/Fear_of_Crime.html?id=1jbFGhBl4hMC))

## Pitfalls/check list

- [ ] 1. not exhaustive?

  - include other option

- [ ] 2. not mutually exclusive?

- [ ] 3. question too long?

  - trim the question by 10% ([King, 2000](https://www.amazon.com/Writing-Memoir-Craft-Stephen-King-ebook/dp/B000FC0SIM)); Even if it is not complex, every word is still time. Keep only necessary words to faster the survey

- [ ] 4. question unbalanced? leading or suggesting an answer?

  - don't forget to mention the other side of information

- [ ] 5. prestige bias (someone people have positive or negative opinion about)?

  - don't unnecessarily include someone who the repondents will support or oppose

    > eg. "How much do you agree" is leading; you should ask "How much do you agree or disagree"

- [ ] 6. universal?

- [ ] 7. double-barreled?

- [ ] 8. poorly ordered?

  > Question order effect:
  >
  > - people want to be consistent
  >
  > - priming: bring to mind things you might not have thought about
  >
  > - affect the interpretation of the following question
  >
  >   [eg](https://www.qualtrics.com/blog/biased-data-is-bad-data-how-to-think-about-question-order/). 55% of Americans said we should let reporters from Soviet Union come in; But when this question was preceeded by the question about whether or not American journalists should be allowed into the Soviet Union, 75% said we should let them come in  
  >   eg. [NLSY97](https://www.nlsinfo.org/sites/nlsinfo.org/files/attachments/121128/nlsy97r1ysaq.html) Q389 asked about specific activities, Q439 and below asked about arrest experience, Q513 asked about perceived arrest rate  
  >   Bad order: being asked about actual arrest experience may remind the respondents and change the perception. This order ties the perceived arrest rate more strongly to actual arrest experience.   
  >   But perceived arrest rate is not going to change the actual arrest experience. Therefore perceived arrest question should come before, actual arrest question comes after

  - global questions should come first, specific questions should come after them

  - ask the outcome variable before the predictors so that you don't prime the predictors ahead of time

  - randomize question order to balance it out 


- [ ] 9. negative and double negative questions?

  - don't use "not" in questions: harder for people to understand negation
  
- [ ] 10. don't use modifying adjectives 

  > eg. "occasionally," "seldom," "occasionally." They are vague and different people have different interpretation

- [ ] 11. when to add don't know option

  - think about how likely people will not know

  > eg. people tend to undecided for voting: better to add don't know option
  >
  > eg. death penalty question: better not to add
  >
  > eg. for voting, add "undecided"

- [ ] 12. when asking about numbers

  - should use standardized scale (numbers are more comparable than likely/unlikely scale) ([Roche et al, 2020](https://journals.sagepub.com/doi/abs/10.1177/0734016820978827?journalCode=cjra))

  - should use benchmark (Ansolabehere, Meredith, & Snowberg, 2013)

    > eg. give historical range and the average value within that range

- [ ] 13. [anchoring vignette](https://gking.harvard.edu/vign/eg/democracy.shtml) (compare to vignette, not used a lot in crim) by [Gary King](https://gking.harvard.edu/files/vign-chinap.pdf)

  > 2 assumptions (often met): response consistency  & stem equivalence (the interpretations of the question stems are the same; respondents only differ in the interpretation of the response options)

  - ensure comparability of responses (to standardize reponses) (Figure 1 in [King et al, 2004](https://dash.harvard.edu/bitstream/handle/1/3965182/King_EnhancingtheValidity.pdf))

  - compare to 2 persons 
    - ordering of the vignette
    
    - don't use direct comparison (one single question), since respondents tend to satisfice ([Hopkins & King, 2011](https://gking.harvard.edu/files/gking/files/implement.pdf))
    
      >  eg. "below Moses, above Moses, or equal to Moses;" almost everyone says equal to
    
  - ask the vignette after or before self-assessment: researchers should move vignettes to before the self-assessments in order to prime and to correct responses ([Hopkins & King, 2011](https://gking.harvard.edu/files/gking/files/implement.pdf))

- [ ] 14. focal answers in numerical questions

  - when you can't use benchmarks you often get focal answers

  - select more than you would expect that people tend to be drawn to (de Bruin, Fischhoff, Millstein, & HalpernFelsher, 2000; Hurd, 2009; [Manski, 2004, p. 18](https://cepr.org/sites/default/files/4797-final.pdf))

    > eg. focal answers: 0, 50, 100, and multiples of 5, normally modal is 50. Numbers like 0 and 100 are not meaningless, they actually have predicted value; (100 represents rounding, 50 means don't know)

- [ ] 15. violating conversational conventions (how people usually talk)?

  - Make the questions sound as natural as possible
  - convention: affirmative first; if against comes before for, it reduces response quality ([Holbrook et al., 2000](https://pprg.stanford.edu/wp-content/uploads/2000-Violating-Conversational-Conventions.pdf)); same with the response option, see [19 in Pitfalss/check list](#pitfallscheck-list). 
    - increase response time
    - increase the number of irrelevant thoughts (distracting)

- [ ] 16. number of response categories: 4 to 5 

  - 5 vs 7: overall quality 0.53 vs 0.39; more categories, less reliable ([Revilla, Saris & Krosnick, 2014](https://pprg.stanford.edu/wp-content/uploads/Choosing-the-Number-of-Categories-in-Agree-Disagree-Scales-Revilla-M.-Saris-W.-Krosnick-J..pdf))
  - unipolar (not satisfied at all to very satisfied) vs bipolar (very dissatisfied to very satisfied): the more options you have, the less reliable  ([Alwin, Baumgartner & Beattie, 2018](https://academic.oup.com/jssam/article-abstract/6/2/212/4201741))

- [ ] 17. information in question stem should reduce social desirability bias

  - quality best when use straightforward/direct questions with naturally ordered response (from affirmative to dis-affirmative)

    >  eg. "some people... other people" ([Yeager & Krosnick, 2012](https://academic.oup.com/poq/article/76/1/131/1894899?login=true))
    >
    >  - average number of words of some-other question is 40; average for other questions is 26; long is bad.
    >  - mentioning some...other causes people to think of the population distribution and to assume it's balanced/evenly split on both sides (distracting)

- [ ] 18. information in response categories should look like population distribution  ([Dillman, Smyth, and Christian, 2014, p. 162](https://www.wiley.com/en-us/Internet%2C+Phone%2C+Mail%2C+and+Mixed+Mode+Surveys%3A+The+Tailored+Design+Method%2C+4th+Edition-p-9781118456149))

  - people assume the options are evenly split
  - people assume the middle one is average

- [ ] 19. stems and response options should match (otherwise response time will increase and error occurs) ([Smyth & Olson, 2019](https://academic.oup.com/jssam/article/7/1/34/4989440?login=true))

  - one of the most common error

  - the phrasing of the question stem should match the ordering of the response options

    > eg. If the response options are agree and disagree, the question stem should be how much do you agree or disagree

  - respondents should know what to expect by reading the question stems

    > eg. Q: **What** is the most important issue? Respondents will consider it as an open-ended question. If you list options, you should ask a question that will be considered as a close-ended one: Q: **Which one of the following** is the most important issue? 



## Redflag words in question wording

- [ ] and (maybe double-barreled)
- [ ] not (respondents may overlook not; negation)
- [ ] or (false dilemma)
- [ ] if (confusing; more difficult to comprehend)

---





# Web survey design settings 【设置】

## Avoid breakoffs

> meta analyses show breakoff rate between 16%-32% 

1. incentives
2. length (the sample - decide - reconsider model)
   - a 20-minute survey has a 20% higher breakoff rate compared to a 10-min one; a 30-minute survey has a 40% higher breakoff rate (See [8 in Basic design](#basic-design))
3. section-transition
   - try not to use too many section transition
   - avoid complicated questions on the first page (otherwise respondents will anticipate burden)
     - no matrix question on first page
     - no multiple and open-ended questions on first page
   - **never use a slider bar**



## Distribute web survey

1. primary ways: URL, QR code, PDF

   - use shortened URL

2. pay attention to the screen resolution and security settings requirement when designing web survey

   - 1% - 5% people have script-disabled, as a result the interactive content cannot be displayed
   - allow the survey to be displayed under the lowest screen resolution possible

3. consistency in display

   - take the survey to all devices/browser/screen/phone to test

4. mobile device

   >  disadvantages: display differently; take the whole screen, force people to scroll, less visibility; takes longer to load
   >
   > eg. a 10-min survey takes 2 min longer to complete on a smartphone compared to desktop; a 20-min web survey takes 6 min longer on a smart phone to complete

   - put on note discouraging participation via mobile device

   > eg. *The survey should take about 15 minutes to complete, but may take much longer if you are using a smartphone*
   >
   > very few respondents do the survey on smartphone when told not to do so (works; 97% followed the direction); some respondents do the survey on computer when told not to do so (about half did it anyway)

5. email invitation

   - subject line

     - don't put recepient's name in the subject line

     - avoid words that will be spam-filtered

       > eg. *Act now; Free; Get paid*

     - use university name

       > eg. *UAlbany University Survey; WSU student experience survey invitation*

   - specify where it comes from

     - include sender name
     - use professional and full name

   - personalize contacts

     >  takes longer but worth it, respondents feel more responsibility and less likely to take it as spam

   - content of the email: URL

   - use 4 - 5 contacts

     - all need to have a link

     - don't need a prenotice

     - change the subject line slightly to make contacts differ

       > eg. last contact: *Thank you (not Urge); Last chance to help WSU; final reminder*

   

## Basic design

1. font and background

   > two concerns:
   >
   > - readability & meaning: legible, easy to read in computer screen; color meaning, infer meaning, contrast issue
   > - contrast between font and background should be strong so that fonts jumps out

   - serif (like Times New Roman; has more detail)vs san serif (like Arial): use san serif fonts that are common
     - Arial: font preference for most people
     - Tahoma: read fast
   - use black font on light color (white) background, use light blue if needs coloring 
   - color fucks up contrast and it communicates meaning: red means passion/anger; yellow can mean warm/sick; blue tends to mean depress/down
   - patterned background not recommended because it distracts people and slow them down

2. header, footer, &  respondent region

   - title in the header
   - contact information in the footer
   - questionnaire stays in the middle (the respondent region), don't deviate from it
   - header, footer, and respondent region should be consistent across the questionnaire

3. page layout & alignment

   > Real-world eye-tracker shows:
   >
   > - the upper left proportion is where the eyes start, stay longest, and linger longest
   > - the bottom right is the least visible part
   > - banner blindness: respondents don't look at the banner again / ignore the banner once start

   - Left line your question; align on top left
   - don't put information in the banner

4. paging (click next for different pages) vs scrolling (one page, scroll down): use paging

   > advantages of paging: 
   >
   > - send every page
   > - easier navigation
   > - give you more control over delivery (like skip pattern, force answering, and question order)
   > - most common design: more than 1 question in a page
   >
   > problems of scrolling:
   >
   > - data are not sent until you finish
   >
   > - increase item nonresponse because people may accidentally scroll past a question
   >
   > - longer completion time (the longer the questionnaire, the smaller the scroll bar, the more precise a movement you would need to get to the exact question you are trying to see)
   >
   > - respondents may change their answer accidentally
   >
   >   > eg. drop box: if you scroll down without clicking outside, your answer changes
   >
   > - if the questionnaire is long, the slide bar will be smaller, it's harder to get to the position accurately
   > - harder to do on smartphone

5. requiring answer or not

   - sometimes necessary for screening questions
   - other than that, shouldn't do it

   > concerns:
   >
   > - if the respondents don't want to answer, they drop or lie
   > - inconsistent with voluntary survey requirement, IRBs don't like it

6. progress indicator (like 25% complete): don't use it 

   > concerns:
   > - it is encouraging based on the assumption that each question takes the same amount of time, but it is often not the case. It is hard to give people feedback that is encouraging
   > - if open-ended question on the first page and there is progress indicator, respondents feel the indicator lies to them and will be pissed off
   > - if it moves fast at the beginning but slowly at the end (hard question at the end): encouraging; less likely to skip, item nonresponse rate decreases, breakoff rate decreases; it motivates people to complete later more difficult tasks ([Conrad et al., 2010](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2910434/))
   > - if it moves slowly at the beginning but fast at the end: discouraging
   > - later meta analyses show the progress indicators do not help and they can hurt ([Villar, Callegaro, and Yang, 2013](https://journals.sagepub.com/doi/10.1177/0894439313497468))

   - can do it in an intermittent way: indicator only pop up in transition
   - without using a progress indicator, the first part of the questionnaire is the key; if the respondents complete the first part faster, they like the survey better, then they stick to it, answer hard questions, and it benefits the whole questionnaire

7. error messages: use them and tailor them to specific questions

   - tell the respondents what they did wrong
   - don't just tell them the answer does not match what we need

8. questionnaire length 

   - web surveys should not be longer than 10 minutes and follow the 15-min ultimate cutoff
   - 8-10 min average
   - have lay persons take the survey, time them to test the length

   > Concerns: longer questionnaire, higher breakoff rate; self report engagement decreases as length increases ([Galesic and Bosnjak, 2009](https://www.jstor.org/stable/25548084?seq=1))



## Question setting

1. selective emphasis and instructions
   - don't use italics to emphasize, italics is harder to read than non-italics
   - don't use underline and color to emphasize, respondents will assume it is a clickable link
   - use **capitalization to emphasize** (helpful, but distracting if used too much); this is a strong convention in web survey
   - use same font and size for question stem and response options, but **balded the stem**, keep the response options not balded 
   - use **italics for instructions** (indicating optional, don't have to read)
   - don't place definition & instructions in navigational path because people will miss it
   - put the instructions between the stems/before the question stem

2. definitions & explanations in case that respondents don't know what the words mean

   - scroll over/click to pop up vs automatically pop up vs base on age vs build into question stem: **put in actual question**

     > advantages: visibility & easy so that people use it more; respondents can be deterred by even a little bit of effort; ensure consistency (different people answer the same question)
     
   - place definition & instructions in the navigational path otherwise people will miss it; don't put them after the question stem and response categories ([Dillman, Smyth, and Christian, 2014, p. 188](https://www.wiley.com/en-us/Internet%2C+Phone%2C+Mail%2C+and+Mixed+Mode+Surveys%3A+The+Tailored+Design+Method%2C+4th+Edition-p-9781118456149))



## Response format & scales

1. strong convention about response types you can use, need to follow it

   - radio button for single choice questions

   - check box for multiple answer questions (check all that apply)

   - Drop boxes with nonvisible answers (like states)

   - never use scroll boxes with some options visible 

     > Concerns: visibility has moderating effects on effects; every format produces some primacy effects; scroll boxes have the biggest primacy effect ([Couper et al., 2004](https://journals.sagepub.com/doi/10.1177/0894439303256555))

1. alignment of response options: vertical vs horizontal vs mixed: use vertical

   - multiple columns artificially increase the response of first option in the second column / the second category in the first row
   - vertical alignment minimizes sideway scroll when displaying on a smartphone

2. visual analog scale vs discrete response scale: use discrete response scale

   > Concerns about visual analog scale ([Funke, 2016](https://journals.sagepub.com/doi/10.1177/0894439315575477)):
   >
   > - takes longer
   > - high breakoff rate when using visual analog scale, increases item missing
   > - require script/flash to display, people may have it blocked
   > - people not familiar with it, therefore don't like it, especially for the older and less educated group

3. running tallies (like sum up to 100): use it to improve the quality of answers

   > still have concerns like arbitrarily adjusting to get 100, but did a better job compared to American Time-Use Survey ([Conrad et al, 2009](https://www.coursera.org/lecture/data-collection-methods/2-3-1-progress-indicators-running-tallies-t8xf9))

4. grid/matrix questions

   - shade out the rows completed to avoid accidentally skipping 
   - shade by the row, not by cell

   > Caution:
   > - faster to do; but can accidentally miss one or click the wrong one
   > - matrix questions are automatically transformed to single questions on smartphones
   > - should consider the effect of different formats; see mobile device instructions in [Distribute web survey](#distribute-web-survey)

5. open-ended questions

   - should avoid because it is harder to type in smart phones

   - size/type of text box affects responses, see open-ended question in [Ensure consistent response](#ensure-consistent-response)

   - provide more information if question stems are easily confusing

     > eg. county vs country: when asking for the name of the county, it is better to specify within US to avoid mistaking it for country, like asking the respondents to provide US county name

6. number effect of scale points

   - don't use numbers, if you have to use numbers, positive numbers go with what people assume; negative numbers cause an artificial effect (don't use)

     > eg. scale from 1 - 5 is different from scale from -2 - 2, respondents tend to go to 2 most positive categories ([Tourangeau, Couper, & Conrad, 2007](https://academic.oup.com/poq/article-abstract/71/1/91/1886357?redirectedFrom=fulltext))

   - don't use color for response options

7. picture: don't use unless there is a theoretical or purposeful reason to do it (don't just do it because you can)

   > Concerns:
   >
   > - pictures have 2 types of effects
   >   - assimilation effect: answer more similar to the picture / go to the direction of the picture
   >   - contrast effect: go different/opposite to the picture 
   >   - eg. compared to a hospital lady, health evaluation increases
   > - recall information that can be inferred from pictures
      > - eg. low frequency vs high frequency effects ([Couper, Tourangeau, & Kenyon, 2004](https://academic.oup.com/poq/article/68/2/255/1826935?login=true))
   > - narrow interpretations


## Response heuristics

[Tourangeau et al., 2013](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3954164/)

- middle means typical/central
- left & top means first
- near means related 
- like (in appearance) means close (in meaning)
- up means good (upper/higher on screen: ranked favorably)
- hierarchy of interpretive cues: some labels outweigh the others (verbal > numerical > color)





